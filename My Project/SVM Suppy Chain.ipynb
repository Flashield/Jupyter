{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMinSuppyChain\n",
    "\n",
    "> 基于支持向量机的供应链绩效评价方法研究\n",
    "\n",
    "支持向量机( Support Vector Machines，简称SVM)理论建立在统计学习理论( Statistical Learning Theory，简称SLT)的结构风险最小化(Structure Risk Minimization，简称SRM)原则之上。其主要思想是针对两类分类问题,在高维空间中寻找一个超平面作为两类的分割，以保证最小的分类错误率。它能够较好地解决小样本、非线性、高维数和局部极小点等实际问题。本章将对它的理论基础、基本思想及研究方向进行介绍。\n",
    "\n",
    "## 4.1概述\n",
    "学习是人类智慧很重要的一个方面,通过对己知事实的分析总结出规律并用其预测不能直接观测得到的事实,这就是学习能力。而学习能力中,最重要的就是对未知的现象做出正确的分析、判断和预测,这称为推广能力。人们对机器智能的研究中,就是希望能够用机器(计算机)来模拟这种学习能力,这称为机器学习问题。\n",
    "\n",
    "> 可以换成机器学习与深度学习\n",
    "\n",
    "SVM理论的理论基础是Vapnik等人提出的统计学习理论。传统的统计学是包括神经网络在内的众多机器学习方法的理论基础，它是训练样本数目趋于无穷大时的渐近理论。但是在现实的问题当中,特别是在SCPM中,样本数目不可能是无穷多的,有时甚至是十分有限的。因此,建立在这一理论基础上的很多优秀学习方法实际表现却可能不尽人意。例如神经网络的过学习(Over Fitting)问题就是一个典型的代表:当样本数目有限时,本来很不错的个学习机器却可能表现出很差的推广能力。统计学习理论建立在一套较坚实的理论基础上。它为解决有限样本学习问题提供了一个统一的框架,它能将很多现有方法纳入其中,有望解决许多原来难以解决的问题。\n",
    "\n",
    "Vanik等人在二十世纪六、七十年代开始致力于小样本情况下机器学习理论的研究,由此发展成为统计学习理论,并在此基础上于1995年提出了SVM理论Vapnik等人给出了统计学习理论的核心概念VC维(Vapnik Chervonenkis Dimension),指出经验风险最小并不能保证期望风险最小,并在此基础上提出了SRM原则。SVM理论建立在统计学习理论的SRM原则之上,是统计学习理论中最新的内容,也是最实用的部分。正因为SVM的提出,才促进了统计学习理论的推广与发展。\n",
    "\n",
    "## 4.2统计学习理论\n",
    "### 4.2.1机器学习问题\n",
    "机器学习的目的是根据给定的己知训练样本求取对系统输入输出之间依\n",
    "赖关系的估计,使它能够对未知输出做出尽可能准确的预测。机器学习问题的\n",
    "基本模型见下图。\n",
    "![机器学习的基本模型](_v_images/20200429111514075_30696.png)\n",
    "机器学习问题可以形式化地表示为:已知变量$y$与输入$x$之间存在一定的未知联合概率$F(x,y)$($x$和$y$之间的确定性关系可以看作是一个特例),机器学习的目的是根据$n$个独立同分布观测样本$(x_1,y_1),(x_2,y_2),…,(x_n,y_n)$,在一组\n",
    "函数$\\{f(x, \\overline \\omega)\\}$中求出一个最优函数$f(x)$使预测的期望风险\n",
    "\n",
    "$$\n",
    "R(\\overline \\omega)=\\int L(y, f(x, \\overline \\omega))dF(x, y) \\label{4.1}\n",
    "$$\n",
    "最小。其中$\\{f(x,\\overline \\omega)\\}$称作预测函数集,$\\overline \\omega\\in \\Omega$为函数的广义参数;$L(y,f(x,\\overline \\omega))$为采用$f(x,\\overline \\omega)$对$y$进行预测而造成的损失。对于两类分类问题,预测函数称作指示函数,损失函数可定义为:\n",
    "\n",
    "$$\n",
    "L(y,f(x,\\overline \\omega))=\\begin{cases} 0 && if & y=f(x,\\overline \\omega) \\\\ 1 && if & y \\neq f(x, \\overline \\omega)\\end{cases}\n",
    "$$\n",
    "\n",
    "### 4.2.2 ERM原则\n",
    "由于分布函数$F(x,y)$未知,因此期望风险无法直接计算和最小化。根据概率论中大数定理的思想,人们想到用算术平均代替式(4.1)中的数学期望,于是定义了:\n",
    "\n",
    "$$\n",
    "R_{emp}(\\overline \\omega)=\\frac 1 n\\sum_{i=1}^n L(y_i,f(x_i,\\overline \\omega)) \\label{4.3}\n",
    "$$\n",
    "来逼近式(4.1)中定义的期望风险,由于$R_{emp}(\\overline \\omega)$是用已知的训练样本(即经验数据)定义的,因此它被称为经验风险。这种用最小经验风险代替求最小期望风险的方法被称为经验风险最小化( Empirical Risk Minimization,简称ERM)原则。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(y,f(x,\\overline \\omega))=\\begin{cases} 0 && if & y=f(x,\\overline \\omega) \\\\ 1 && if & y \\neq f(x, \\overline \\omega)\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
