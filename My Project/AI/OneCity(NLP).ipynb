{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中移集成（雄安产业研究院）首届OneCity编程大赛\n",
    "\n",
    "\n",
    "竞赛地址：\n",
    "https://www.dcjingsai.com/v2/cmptDetail.html?id=457\n",
    "\n",
    "参考资料地址：\n",
    "https://www.kesci.com/mw/project/5be7e948954d6e0010632ef2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets_PATH = '~/OneDrive/ProjectSpace/Jupyter/datasets/'\n",
    "TrainSets_filename = 'answer_train.csv'\n",
    "TestSets_filename = 'submit_example_test1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Data = pd.read_csv(Datasets_PATH+TrainSets_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/市管宗教活动场所.csv</td>\n",
       "      <td>文化休闲</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/南涧彝族自治县市汶上县残疾人联合会_残疾人证办理_.csv</td>\n",
       "      <td>医疗卫生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/价格监测信息公开事项汇总信息_.xls</td>\n",
       "      <td>经济管理</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/县级公共图书馆文化馆总分馆制信息_大安市.csv</td>\n",
       "      <td>文化休闲</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/县级公共图书馆文化馆总分馆制信息_陇南市武都区.csv</td>\n",
       "      <td>文化休闲</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>train/二季度行政处置状况公告信息_.xls</td>\n",
       "      <td>文秘行政</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>train/社保_机关事业单位离退休人员及离休遗属增减，离退休费、离休人员抚恤金、丧葬费及遗...</td>\n",
       "      <td>劳动人事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>train/托里县农林牧渔业产值.csv</td>\n",
       "      <td>农业畜牧业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>train/设立经营性互联网文化单位审批_忻州市.csv</td>\n",
       "      <td>文化休闲</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>train/市本级事业公司信息.csv</td>\n",
       "      <td>劳动人事</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filename  label\n",
       "0                                     train/市管宗教活动场所.csv   文化休闲\n",
       "1                    train/南涧彝族自治县市汶上县残疾人联合会_残疾人证办理_.csv   医疗卫生\n",
       "2                              train/价格监测信息公开事项汇总信息_.xls   经济管理\n",
       "3                         train/县级公共图书馆文化馆总分馆制信息_大安市.csv   文化休闲\n",
       "4                      train/县级公共图书馆文化馆总分馆制信息_陇南市武都区.csv   文化休闲\n",
       "...                                                  ...    ...\n",
       "59995                           train/二季度行政处置状况公告信息_.xls   文秘行政\n",
       "59996  train/社保_机关事业单位离退休人员及离休遗属增减，离退休费、离休人员抚恤金、丧葬费及遗...   劳动人事\n",
       "59997                               train/托里县农林牧渔业产值.csv  农业畜牧业\n",
       "59998                       train/设立经营性互联网文化单位审批_忻州市.csv   文化休闲\n",
       "59999                                train/市本级事业公司信息.csv   劳动人事\n",
       "\n",
       "[60000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Raw_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['文化休闲', '医疗卫生', '经济管理', '教育科技', '城乡建设', '工业', '民政社区', '交通运输',\n",
       "       '生态环境', '政法监察', '农业畜牧业', '文秘行政', '劳动人事', '资源能源', '信息产业', '旅游服务',\n",
       "       '商业贸易', '气象水文测绘地震地理', '财税金融', '外交外事'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Raw_Data.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.168 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.enable_parallel() #并行分词开启\n",
    "Raw_Data['文本分词'] = Raw_Data['filename'].apply(lambda i:jieba.cut(i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Data['文本分词'] =[' '.join(i) for i in Raw_Data['文本分词']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>文本分词</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/市管宗教活动场所.csv</td>\n",
       "      <td>文化休闲</td>\n",
       "      <td>train / 市管 宗教 活动场所 . csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/南涧彝族自治县市汶上县残疾人联合会_残疾人证办理_.csv</td>\n",
       "      <td>医疗卫生</td>\n",
       "      <td>train / 南涧彝族自治县 市 汶上县 残疾人 联合会 _ 残疾人 证 办理 _. csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/价格监测信息公开事项汇总信息_.xls</td>\n",
       "      <td>经济管理</td>\n",
       "      <td>train / 价格 监测 信息 公开 事项 汇总 信息 _. xls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/县级公共图书馆文化馆总分馆制信息_大安市.csv</td>\n",
       "      <td>文化休闲</td>\n",
       "      <td>train / 县级 公共 图书馆 文化馆 总分 馆制 信息 _ 大安市 . csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/县级公共图书馆文化馆总分馆制信息_陇南市武都区.csv</td>\n",
       "      <td>文化休闲</td>\n",
       "      <td>train / 县级 公共 图书馆 文化馆 总分 馆制 信息 _ 陇南 市 武都 区 . csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename label  \\\n",
       "0                   train/市管宗教活动场所.csv  文化休闲   \n",
       "1  train/南涧彝族自治县市汶上县残疾人联合会_残疾人证办理_.csv  医疗卫生   \n",
       "2            train/价格监测信息公开事项汇总信息_.xls  经济管理   \n",
       "3       train/县级公共图书馆文化馆总分馆制信息_大安市.csv  文化休闲   \n",
       "4    train/县级公共图书馆文化馆总分馆制信息_陇南市武都区.csv  文化休闲   \n",
       "\n",
       "                                               文本分词  \n",
       "0                          train / 市管 宗教 活动场所 . csv  \n",
       "1   train / 南涧彝族自治县 市 汶上县 残疾人 联合会 _ 残疾人 证 办理 _. csv  \n",
       "2               train / 价格 监测 信息 公开 事项 汇总 信息 _. xls  \n",
       "3        train / 县级 公共 图书馆 文化馆 总分 馆制 信息 _ 大安市 . csv  \n",
       "4  train / 县级 公共 图书馆 文化馆 总分 馆制 信息 _ 陇南 市 武都 区 . csv  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Raw_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"对数损失度量（Logarithmic Loss  Metric）的多分类版本。\n",
    "    :param actual: 包含actual target classes的数组\n",
    "    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(Raw_Data.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(Raw_Data.文本分词.values, y, \n",
    "                                                  stratify=y,\n",
    "                                                  random_state=55, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train / 盘锦市 盘山县 民声 阅办 记录 信息 _. csv',\n",
       "       'train / 长岛 国家级 自然保护区 管理局 年 新闻宣传 状况 总括 表 _. xls',\n",
       "       'train / 淮安市 旅行社 目录 . csv', ...,\n",
       "       'train / 新宾满族自治县 县 法人 行政处罚 数据 . csv',\n",
       "       'train / 太原市 杏花岭区 市 广东省 名牌产品 工业 类 称号 信息 _. xls',\n",
       "       'train / 山西省 长治市 黎城县 县 法人 行政处罚 数据 . csv'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_normalizer(tokens):\n",
    "    \"\"\" 将所有数字标记映射为一个占位符（Placeholder）。\n",
    "    对于许多实际应用场景来说，以数字开头的tokens不是很有用，\n",
    "    但这样tokens的存在也有一定相关性。 通过将所有数字都表示成同一个符号，可以达到降维的目的。\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super(NumberNormalizingVectorizer, self).build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用刚才创建的NumberNormalizingVectorizer类来提取文本特征，注意里面各类参数的含义，自己去sklearn官方网站找教程看\n",
    "\n",
    "stwlist=[line.strip() for line in open('/home/kesci/input/stopwords7085/停用词汇总.txt',\n",
    "                                       'r',encoding='utf-8').readlines()]\n",
    "\n",
    "tfv = NumberNormalizingVectorizer(min_df=3,  \n",
    "                                  max_df=0.5,\n",
    "                                  max_features=None,                 \n",
    "                                  ngram_range=(1, 2), \n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  stop_words = stwlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用刚才创建的NumberNormalizingVectorizer类来提取文本特征，注意里面各类参数的含义，自己去sklearn官方网站找教程看\n",
    "stwlist=[line.strip() for line in open('/home/zhuangbin/OneDrive/ProjectSpace/Jupyter/datasets/baidu_stopwords.txt','r',encoding='utf-8').readlines()]\n",
    "\n",
    "tfv = NumberNormalizingVectorizer(min_df=3,  \n",
    "                                  max_df=0.5,\n",
    "                                  max_features=None,                 \n",
    "                                  ngram_range=(1, 2), \n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  stop_words = stwlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuangbin/pyenv/jupyter/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NumberNormalizingVectorizer(max_df=0.5, min_df=3, ngram_range=(1, 2),\n",
       "                            stop_words=['--', '?', '“', '”', '》', '－－', 'able',\n",
       "                                        'about', 'above', 'according',\n",
       "                                        'accordingly', 'across', 'actually',\n",
       "                                        'after', 'afterwards', 'again',\n",
       "                                        'against', \"ain't\", 'all', 'allow',\n",
       "                                        'allows', 'almost', 'alone', 'along',\n",
       "                                        'already', 'also', 'although', 'always',\n",
       "                                        'am', 'among', ...])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "NumberNormalizingVectorizer(max_df=0.5, min_df=3, ngram_range=(1, 2),\n",
       "                            stop_words=['--', '?', '“', '”', '》', '－－', 'able',\n",
       "                                        'about', 'above', 'according',\n",
       "                                        'accordingly', 'across', 'actually',\n",
       "                                        'after', 'afterwards', 'again',\n",
       "                                        'against', \"ain't\", 'all', 'allow',\n",
       "                                        'allows', 'almost', 'alone', 'along',\n",
       "                                        'already', 'also', 'although', 'always',\n",
       "                                        'am', 'among', ...])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用TF-IDF来fit训练集和测试集（半监督学习）\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "\n",
    "# 使用TF-IDF来fit训练集和测试集（半监督学习）\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用提取的TFIDF特征来fit一个简单的Logistic Regression \n",
    "clf = LogisticRegression(C=1.0,solver='lbfgs',multi_class='multinomial')\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuangbin/pyenv/jupyter/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.5, min_df=3, ngram_range=(1, 2),\n",
       "                stop_words=['--', '?', '“', '”', '》', '－－', 'able', 'about',\n",
       "                            'above', 'according', 'accordingly', 'across',\n",
       "                            'actually', 'after', 'afterwards', 'again',\n",
       "                            'against', \"ain't\", 'all', 'allow', 'allows',\n",
       "                            'almost', 'alone', 'along', 'already', 'also',\n",
       "                            'although', 'always', 'am', 'among', ...])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctv = CountVectorizer(min_df=3,\n",
    "                      max_df=0.5,\n",
    "                      ngram_range=(1,2),\n",
    "                      stop_words = stwlist)\n",
    "\n",
    "# 使用Count Vectorizer来fit训练集和测试集（半监督学习）\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用提取的word counts特征来fit一个简单的Logistic Regression \n",
    "clf = LogisticRegression(C=1.0,solver='lbfgs',multi_class='multinomial')\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用提取的TFIDF特征来fitNaive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用提取的word counts特征来fitNaive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用SVD进行降维，components设为120，对于SVM来说，SVD的components的合适调整区间一般为120~200 \n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "#对从SVD获得的数据进行缩放\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用下SVM模型\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=10, nthread=10, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.301 \n"
     ]
    }
   ],
   "source": [
    "# 基于tf-idf特征，使用xgboost\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=10, nthread=10, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.304 \n"
     ]
    }
   ],
   "source": [
    "# 基于word counts特征，使用xgboost\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "#print(classification_report(predictions, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Test = pd.read_csv(Datasets_PATH+TestSets_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.enable_parallel() #并行分词开启\n",
    "Raw_Test['文本分词'] = Raw_Test['filename'].apply(lambda i:jieba.cut(i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Test['文本分词'] =[' '.join(i) for i in Raw_Test['文本分词']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Test['文本分词']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ctv =  ctv.transform(Raw_Test['文本分词']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = clf.predict(X_test_ctv.tocsc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rlt = lbl_enc.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rlt.tofile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test_rlt).to_csv(Datasets_PATH+'rlt_onecity_001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Test['result'] = pd.DataFrame(y_test_rlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Test.to_csv(Datasets_PATH+'submit_result_201113_01.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_Test[['filename','result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
